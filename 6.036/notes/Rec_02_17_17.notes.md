# Recitation 2
## Agenda
- Linear classification
- perception
- loss function
- SVM
- gradient descent
    - pegasos?

## Linear classification
- Splits space into two
    - if you're on one side of line, you're positive
    - other side is negative
- more formally
    - h(x, theta) = sign(dot(theta, x) + theta_0)

### Perception
Online learning algorithm
Linear Classification
Data has to be linearly seperable

- input:
    - traininings set
        - feature vectors, x
        - classifications
    - epoch
        - tells how many times to iterate through data
            - could iterate until its converged or use epoch
- output:
    - classsifier, described by
        - theta
        - theta_0

#### Algorithm

```
theta_1, theta_0 = 0
for t=1 ... T, number of epochs
    for i=1 ... n
        if y^i * (dot(theta, x^i) + theta_0) <= 0, mistake in classsifier when classifier matches label
            theta <-- theta + dot(y^i, x^i)
            theta_0 <-- theta_0 + y^i
```

First iteration through perceptron will always make mistake

#### Perceptron Example
|  i | x^i  | y^i |
| ---| ---  | --- |
| 1  |(5,1) | +1  |
| 2  |(1,3) | -1  |
| 3  |(-1,1)| -1  |

i = 0, mistake is True  
theta = Transpose([0, 0])  
theta <-- Transpose([5, 1])  

i = 1, mistake is True  
theta = Transpose([5, 1])  
theta <-- Transpose([5, 1]) - Transpose([1,3]) = Transpose([4, -2])  

i = 2, mistake if false  
theta = Transpose([0, 0])  

```
Mistakes to converge <= (R/gamma)^2  
```


## Loss
### Zero One loss function
prediction correct = loss of 0
prediction incorrect = loss of 1
- downside
    - if a point is right next to ur decision boundary and u get it wrong its
        not as bad as if you classified a point that should be extremely one
        way or another, but with zero one loss, they both are loss of 1

### Hinge Loss

## SVM (Support Vector Machine)
Keys to SVM:
1. minimize average Loss_h (hinge loss)
2. maximize margins
    - maximize `1/||theta||`
    - to maximize, minimize theta
