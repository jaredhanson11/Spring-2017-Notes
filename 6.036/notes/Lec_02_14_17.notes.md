# 6.036 Lecture 3
02/14/17
## Linear Classifications

### Formalize ML Problems:
#### Set up
- Label/Outputs:
    - `x in set({1,-1})`
- Feature vectors/Inputs:
    - `vec(x) = Transpose([x_1, x_2 ... x_d] in R^d)`
        - each `x_i` in is value for input
- Training set:
    - `S_n = {(x^i, y^i), i=1...n}`
    - `y^i is the classifier, corresponding to label output`

#### Solve for classifier
- classifier/hypothesis/seperator
    - `h: R^d --> {1, -1`}
- set of classifiers
    - `h in set(H)`
- __training error__:
    - number times classifier is wrong over training set
- __test/generalization error__:
    - training error over set of classifiers

- __classifier__
    - `h(vec(x); theta, theta_0) = sign(dot(theta, vec(x)) + theta_0) = {+1 if >0, -1 if <0`
    - __decision boundary__ of classifier
        - equal to `vec(x):` where `dot(theta, vec(x)) + theta_0 = 0`

#### Linear Seperation
- can a line split up the training set, by classifications, perfectly
- in 2D it seems hard to see why linear seperation is useful
    - 4 points in 2D can be such that there is not linear seperation
- as you introduce more dimensions, your 'classifier dimension' grows too
    - increasing dimensions of vector makes linear classifier much more
        powerful

##### Perception Algorithm
- input: `S_n = {(x^i, y^i), i=1...n}`
- output: `theta, theta_0`
- alg:
```
vec(theta) = 0, vec(theta_0)=0
for set in S_n
    if y(dot(vec(theta), vec(theta_0)) < 0 //mistake
        vec(theta) <-- vet(theta) + y^i*vec(x^i)
        new vec(theta) is old vec(theta) plus classifier (right or wrong{-1, 1}) times vec(training point)
        vec(theta_0) <-- vet(theta_0) + y^i
```
- limit the number of times you update `vector(theta)`
- linear seperator is orthogonal to theta

###### Linear seperation with margin
- now instead of just seperating training set, seperate with a margin with
    length `gamma`
    - ` y^i(dot(vec(theta),x^i) / ||vec(theta)|| > gamma`
    - applies both to both negative and positive classifiers

##### Perception Algorithm Convergence
__Determine if linear seperation with margin is valid?__  
for all i:
1. ` y^i(dot(vec(theta),x^i) / ||vec(theta)|| > gamma`
    - doesn't tell much though because can always just scale axes up
2. `||x^i|| < R`
    - `R` is radius
    - puts constraint on set so can't just scale axes

If 1 and 2 hold true for all i, perceception algorithm converges in at most `R**2 / gamma**2`



