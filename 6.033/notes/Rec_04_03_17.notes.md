# Lecture 14: Start of failures

- Reliability via Replication
    - General appraoch to building fault-tolerance systems
    - Single-disk failures: RAID

## How to design fault tolerant systems

1. *identify* all possible faults
2. *detect* and *contain* the faults
    - TCP example: checks original hash is same as hash of data sent
3. *handle* the fault
    - possible ways of handling:
        1. do nothing
        2. "fail fast" aka escalate error to calling module right when it breaks
        3. "fail stop" stop immediately upon detection
        4. mask/hide error

## Quantifying Reliability

- __MTTF__: mean time to fail
- __MTTR__: mean time to repair
- __Availability__: MTTF/(MTTF + MTTR)
- Example of quantifying reliability:
    1. computer crashes once a month:
        - MTTF = 43,200 minutes (mins in a month)
    2. computer takes 10 minutes to restart after crashing
        - MTTR = 10 mins
    3. calculate availability from that
        - Availability: 43,200 / 43,210 = 99.9% availabile

## Dealing with disk failures

- unlike other components of computer, if disk fails, your data is lost.
    - other components performing functions can be replaced, but data can't be replaced
- for that reason, the cost of disk failures are the highest
- problems measuring disk uptime comes from the tests being done
- probability of disks failing:
    - right at beginning high risk of failure: "infant mortaility"
    - middle aged disk: "stable operating period"
    - greater than 5 years or so: "burn out"

only concerned with entire disk failures for sake of data centers etc. not
personal computer failures

## RAID: Suite of Failure recovery

- generally not used across machines. they'll all belong to a single machine
- doesn't help when power to entire array of disks goes out or failures that
    have some sort of dependencies
- need to consider writes and backups that happen across the network, so take
    into account network failures etc.
- locks/concurrency can't fix it in a performant practical way

- trade-offs of fault tolerat systems:
    - reliablity comes at cost:
    - reliability
    - monetary cost
    - simplicity
    - reliability

### RAID 1: Backing up disk

__Process:__ have two disks

1. When writing to disk 1, write copy to disk 2 also
2. when reading read from disk 1, or disk 2
3. if disk 1 fails, copy from disk 2 to disk 1

- trade-offs:
    1. writes are slower
    2. reads are potentially faster (parallel reading)
    3. costs a lot of money for a disk, for no added data storage

    - __+__ :
        - can recover from single disk failures
    - __-__ :
        - requires 2N disks where N is number disks trying to back up

### RAID 4: dedicated parity disk

- RAID 2 and 3 are really never used, RAID 4 really isn't used either
- __TODO__ learn more about implmentation of RAID 4 and how it works
- bunch of XORs in parity disk
    - using parity disk and xors to recover from failures
- raid 4 actually takes performance into account unlike RAID 1,2, or 3
    especially in read phase

- trade-offs:
    - __+__ :
        - can recover from single disk failure
        - requires N+1 disks (not 2N) for parity disk for recovery
        - performance benefits: stripe a single file across mulitple data disks
    - __-__:
        - all writes have to hit the parity disk: __problem solved by RAID 5__

### RAID 5: spread out the parity disk

- exact same as RAID 4 but spreads out the parity differently

- trade-offs:
    - same as RAID 4 except the negative turns to positive such that
    - all writes are spread across disks now, not all rely on one parity disk


